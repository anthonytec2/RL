Timer unit: 1e-06 s

Total time: 7.62515 s
File: DQN.py
Function: create_new_batch at line 13

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    13                                           @profile
    14                                           def create_new_batch(env, frame_skip, act=-1):
    15        45         36.0      0.8      0.0      img_ls = []
    16        45         31.0      0.7      0.0      if act == -1:
    17        50         67.0      1.3      0.0          for i in range(frame_skip):
    18        40    2126609.0  53165.2     27.9              img = env.render(mode='rgb_array')
    19        40     942410.0  23560.2     12.4              img_ls.append(cnn.composed(img))
    20        40       2404.0     60.1      0.0              env.step(i % 2)
    21                                               else:
    22       175        252.0      1.4      0.0          for i in range(frame_skip):
    23       140    1218433.0   8703.1     16.0              img = env.render(mode='rgb_array')
    24       140    3320711.0  23719.4     43.5              img_ls.append(cnn.composed(img))
    25       140       8136.0     58.1      0.1              env.step(act)
    26        45       6065.0    134.8      0.1      return torch.stack(img_ls)

Total time: 7.9e-05 s
File: DQN.py
Function: bound_reward at line 29

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    29                                           @profile
    30                                           def bound_reward(reward):
    31        45         40.0      0.9     50.6      if reward == 0:
    32         7          2.0      0.3      2.5          return 0
    33        38         23.0      0.6     29.1      elif reward > 0:
    34        38         14.0      0.4     17.7          return 1
    35                                               elif reward < 0:
    36                                                   return -1

Total time: 2.4e-05 s
File: DQN.py
Function: eps_anneal at line 39

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    39                                           @profile
    40                                           def eps_anneal(epsilon):
    41        10         10.0      1.0     41.7      if epsilon < .1:
    42                                                   epsilon = .1
    43                                               else:
    44        10         10.0      1.0     41.7          epsilon -= (1-.1)/(1e3)
    45        10          4.0      0.4     16.7      return epsilon

Total time: 8.78281 s
File: DQN.py
Function: main at line 48

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    48                                           @profile
    49                                           def main():
    50         1          9.0      9.0      0.0      D = 1e6  # Amnt in replay memory
    51         1          2.0      2.0      0.0      M = 10  # Number of epsiodes to run
    52         1          1.0      1.0      0.0      T = 50  # Number of steps to take maximum
    53         1          1.0      1.0      0.0      epsilon = 1  # Choose random actions prob
    54         1          2.0      2.0      0.0      gamma = .9  # Forgetting factor of the past
    55         1          2.0      2.0      0.0      batch_size = 32  # Number of elements to sample from replay memory
    56         1          1.0      1.0      0.0      num_frames = 0  # Counter for the number of frames
    57         1          2.0      2.0      0.0      frame_skip = 4  # Number of frames to wait before selecting a new action
    58         1      33045.0  33045.0      0.4      env = gym.make('CartPole-v0')
    59         1          6.0      6.0      0.0      er = erp.experience_replay(D)
    60         1          1.0      1.0      0.0      reward_ls = []
    61         1        877.0    877.0      0.0      env.reset()
    62         1     127730.0 127730.0      1.5      writer = SummaryWriter()
    63         1          4.0      4.0      0.0      writer.add_graph(cnn.model, torch.autograd.Variable(
    64         1      26816.0  26816.0      0.3          torch.Tensor(4, 1, 84, 84)))
    65        11         21.0      1.9      0.0      for eps in range(M):
    66        10        274.0     27.4      0.0          env.reset()
    67        10    3077471.0 307747.1     35.0          batch = create_new_batch(env, frame_skip, act=-1)
    68        10         16.0      1.6      0.0          reward_gl = 0
    69        10         10.0      1.0      0.0          max_q = 0
    70        10        331.0     33.1      0.0          print(eps, epsilon, num_frames)
    71        45         84.0      1.9      0.0          for t in range(T):
    72        45        126.0      2.8      0.0              if random.random() < epsilon:
    73        45       1786.0     39.7      0.0                  act = env.action_space.sample()
    74                                                       else:
    75                                                           with torch.no_grad():
    76                                                               Q = cnn.model(batch)
    77                                                               if torch.max(Q) > max_q:
    78                                                                   max_q = torch.max(Q).item()
    79                                                               act = torch.argmax(Q).item()
    80        45       1754.0     39.0      0.0              _, reward, done, _ = env.step(act)
    81        45         71.0      1.6      0.0              num_frames += frame_skip
    82        45        427.0      9.5      0.0              reward = bound_reward(reward)
    83        45         59.0      1.3      0.0              reward_gl += reward
    84        45         74.0      1.6      0.0              reward_ls.append(reward)
    85        45         58.0      1.3      0.0              if done:
    86        10         70.0      7.0      0.0                  er.add_mem(batch, act, reward, False)
    87                                                       else:
    88        35    4558910.0 130254.6     51.9                  new_batch = create_new_batch(env, frame_skip, act=act)
    89        35        452.0     12.9      0.0                  er.add_mem(batch, act, reward, new_batch)
    90        35         48.0      1.4      0.0                  batch = new_batch
    91
    92        45         79.0      1.8      0.0              if len(er.replay) > batch_size:
    93        13        356.0     27.4      0.0                  cnn.optimizer.zero_grad()
    94        13       1934.0    148.8      0.0                  rnd_mini_batch = er.sample_batch(32)
    95       429        995.0      2.3      0.0                  for memory in rnd_mini_batch:
    96       416       1513.0      3.6      0.0                      if hasattr(memory.future_state, "shape"):
    97       328       2004.0      6.1      0.0                          with torch.no_grad():
    98       328        454.0      1.4      0.0                              y = memory.reward+gamma * \
    99       328     210039.0    640.4      2.4                                  torch.max(cnn.model(memory.future_state))
   100                                                               else:
   101        88        238.0      2.7      0.0                          y = memory.reward
   102       416     275272.0    661.7      3.1                      loss = torch.sum((y-cnn.model(memory.state))**2)
   103       416     448468.0   1078.0      5.1                      loss.backward()
   104        13       1284.0     98.8      0.0                  cnn.optimizer.step()
   105        45         56.0      1.2      0.0              if done:
   106        10         15.0      1.5      0.0                  break
   107        10       1233.0    123.3      0.0          writer.add_scalar('data/eps_len', t, num_frames)
   108        10        565.0     56.5      0.0          writer.add_scalar('data/q', t, max_q)
   109        10        459.0     45.9      0.0          writer.add_scalar('data/reward', reward_gl, num_frames)
   110        10        418.0     41.8      0.0          writer.add_scalar('data/eps', epsilon, num_frames)
   111        10         99.0      9.9      0.0          epsilon = eps_anneal(epsilon)
   112         1       6536.0   6536.0      0.1      env.close()
   113         1        254.0    254.0      0.0      writer.close()