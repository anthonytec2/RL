{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cart Pole Reinforcment Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will go over the cart pole example in Open AI Gym. Gym is a library of many different environments where AI can be created to optimize a task. The [cart pole](https://github.com/openai/gym/wiki/CartPole-v0) tasks goal is to balance a rod just like the case of an inverted pendulum problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make the gym environment where you can specify whatever environment you would like to use. In this environment every frame we can pick an action to perform and see the observations as a result of this action. Through the env action space one can see that there are 2 discrete actions that can occur for this example going right(1) or going left(0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "The Number of actions are Discrete(2)\n",
      "The Data Types of actions are int64\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print(f'The Number of actions are {env.action_space}')\n",
    "print(f'The Data Types of actions are {env.action_space.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup a monitor which will record the results of the actions taken. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.wrappers.monitor.Monitor(env, '/home/anthony/Documents/cart_pole/', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset will set the environment to an initial state from which you can perform various actions. Initially reset will return to the initial observation state. These different observation states are: 0) Cart Position 1) Cart Velocity 3) Pole Angle 4) Pole Velocity Tip. For each of these actions a reward is given if the cart has not gone into a terminal state. This terminal state is triggered when the pole angle is greater than $12^\\circ$, the cart position is out of the display or the episode length ranges past 200. The goal in the end is to get a reward greater than 195 over 100 trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0157479 , -0.00429214, -0.03903011, -0.04250227])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bellow is an example of a loop for performing actions and rendering the results. For each step function the observations, reward, termination boolean and meta information is returned. The input to the step function is the given action to take, for this example a random action is taken so the environment randomly samples from the possible action space. After performing this action one can render the results to see things occur with time. If the termination criteria is met then the simulation is terminated, else it continues performing another random action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "Step 2\n",
      "Step 3\n",
      "Step 4\n",
      "Step 5\n",
      "Step 6\n",
      "Step 7\n",
      "Step 8\n",
      "Step 9\n",
      "Step 10\n",
      "Step 11\n",
      "Step 12\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,100):\n",
    "    print(f'Step {i}')\n",
    "    ob, reward, done, meta=env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will cleanup the environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
